---
layout: single
author_profile: true
---
I was a co-founder of [Adept](https://www.adept.ai/) and am now a scientist at Amazon AGI.

I led the [Scratchpad](https://arxiv.org/abs/2112.00114) project ([tweet](https://x.com/Maxwell_Nye/status/1466229884385177612)), inventing a technique for enabling transformers to perform multi-step reasoning (now commonly known as chain of thought), which is a foundation of many [modern](https://openai.com/o1/) [AI](https://ai.google.dev/gemini-api/docs/thinking) [systems](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-of-thought). I jointly hold the [patent for chain of thought](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=NsuX8R8AAAAJ&cstart=20&pagesize=80&citation_for_view=NsuX8R8AAAAJ:MXK_kJrjxJIC).

During my PhD, I spent time at Google Brain, working with [Augustus Odena](https://www.augustusodena.com/). In addition to Scratchpad, we also did some of the early work using large language models to automatically [write programs](https://arxiv.org/abs/2108.07732).

I received my PhD from MIT. I was advised by [Josh Tenenbaum](https://web.mit.edu/cocosci/josh.html) and [Armando Solar-Lezama](https://people.csail.mit.edu/asolar/).
I also worked with [Brenden Lake](https://cims.nyu.edu/~brenden/) (NYU/Facebook AI) and [Jacob Andreas](https://www.mit.edu/~jda/).
My research focus at MIT was program synthesis: using deep learning as well as symbolic techniques to automatically write programs. 

I've also spent time as an intern at Facebook AI Research.

Before MIT, I studied physics at Harvard.

## Publications
An up-to-date list may be found at [Google Scholar](https://scholar.google.com/citations?user=NsuX8R8AAAAJ&hl=en)  
- [**Show Your Work: Scratchpads for Intermediate Computation with Language Models**](https://arxiv.org/pdf/2112.00114.pdf)  
**Maxwell Nye**, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, Augustus Odena.  
2021\.

- [**Program Synthesis with Large Language Models.**](https://arxiv.org/pdf/2108.07732.pdf)  
Jacob Austin\*, Augustus Odena\*, **Maxwell Nye**, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, Charles Sutton.  
2021\. [[data]](https://github.com/google-research/google-research/tree/master/mbpp)

- [**Implicit Representations of Meaning in Neural Language Models.**](https://arxiv.org/pdf/2106.00737.pdf)  
Belinda Z. Li, **Maxwell Nye**, Jacob Andreas.  
ACL 2021. [[code]](https://github.com/belindal/state-probes)

- [**Improving Coherence and Consistency in Neural Sequence Models with Dual-System, Neuro-Symbolic Reasoning.**](https://arxiv.org/pdf/2107.02794.pdf)  
**Maxwell Nye**, Michael Henry Tessler, Josh Tenenbaum, Brenden Lake.  
NeurIPS 2021.

- [**Communicating Natural Programs to Humans and Machines.**](https://arxiv.org/pdf/2106.07824.pdf)  
Samuel Acquaviva, Yewen Pu, Marta Kryven, Catherine Wong, Gabrielle E Ecanow, **Maxwell Nye**, Theodoros Sechopoulos, Michael Henry Tessler, Josh Tenenbaum.  
In submission. [[data]](https://github.com/samacqua/LARC)

- [**A Large-Scale Benchmark for Few-Shot Program Induction and Synthesis.**](http://proceedings.mlr.press/v139/alet21a/alet21a.pdf)  
Ferran Alet\*, Javier Lopez-Contreras\*, James Koppel, **Maxwell Nye**, Armando Solar-Lezama, Tomas Lozano-Perez, Leslie Kaelbling, Josh Tenenbaum.  
ICML 2021. Spotlight.

- [**Representing Partial Programs with Blended Abstract Semantics.**](https://arxiv.org/pdf/2012.12964.pdf)  
**Maxwell Nye**, Yewen Pu, Matthew Bowers, Jacob Andreas, Josh Tenenbaum, Armando Solar-Lezama.  
ICLR 2021.
Also presented at [NeurIPS 2020 Workshop on Computer-Assisted Programming](https://capworkshop.github.io/). 

- [**DreamCoder: Growing generalizable, interpretable
knowledge with wake-sleep Bayesian program learning.**](https://arxiv.org/pdf/2006.08381.pdf)  
Kevin Ellis, Catherine Wong, **Maxwell Nye**, Mathias Sabl√©-Meyer, Luc Cary, Lucas Morales, Luke Hewitt, Armando Solar-Lezama, Josh Tenenbaum.  
PLDI 2021.

- [**Learning Compositional Rules via Neural Program Synthesis.**](https://arxiv.org/pdf/2003.05562.pdf)  
**Maxwell Nye**, Armando Solar-Lezama, Josh Tenenbaum, Brenden Lake.  
NeurIPS 2020. [[code]](https://github.com/mtensor/rulesynthesis)  
Also presented at [NeurIPS 2019 Workshop on Context and Compositionality](https://context-composition.github.io/) as a [spotlight talk](https://slideslive.com/38922749/learning-compositional-rules-via-neural-program-synthesis).

- [**Write, Execute, Assess: Program Synthesis with a REPL.**](https://arxiv.org/pdf/1906.04604.pdf)  
Kevin Ellis\*, **Maxwell Nye\***, Yewen Pu\*, Felix Sosa\*, Josh Tenenbaum, Armando Solar-Lezama.  
NeurIPS 2019.

- [**Learning to Infer Program Sketches.**](https://arxiv.org/pdf/1902.06349.pdf)  
**Maxwell Nye**, Luke Hewitt, Josh Tenenbaum, Armando Solar-Lezama.  
ICML 2019. [[code]](https://github.com/mtensor/neural_sketch)  
**Press:** [MIT News](http://news.mit.edu/2019/toward-artificial-intelligence-that-learns-to-write-code-0614)

- [**The Variational Homoencoder: Learning to Infer High-Capacity Generative Models from Few Examples.**](https://arxiv.org/pdf/1807.08919.pdf)  
Luke Hewitt, **Maxwell Nye**, Andreea Gane, Tommi Jaakkola, Josh Tenenbaum.   
UAI 2018, oral presentation. [[code]](https://github.com/insperatum/vhe)

- [**Are Efficient Deep Representations Learnable?**](https://arxiv.org/pdf/1807.06399.pdf)  
**Maxwell Nye**, Andrew Saxe.  
ICLR 2018, workshop.

*equal contribution